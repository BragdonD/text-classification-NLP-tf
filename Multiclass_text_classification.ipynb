{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BragdonD/text-classification-NLP-tf/blob/main/Multiclass_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKFNPb5U6OGe"
      },
      "source": [
        "Import the depencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tensorflow_addons\n",
        "%pip install tensorflow-model-analysis"
      ],
      "metadata": {
        "id": "aVeO_XO7SOAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwAtmNdd5iuw",
        "outputId": "00beb6e9-b526-419d-df25-77f4289e290c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout, Bidirectional, GRU, Conv1D, GlobalMaxPooling1D,BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.data import Dataset\n",
        "from tensorflow.keras import regularizers\n",
        "#import tensorflow_addons as tfa\n",
        "#import tensorflow_model_analysis as tfma\n",
        "import tensorflow as tf\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('words')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0P7WgFs6dBX"
      },
      "source": [
        "Mouting the drive and defining the path toward the dataset file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz-Ds6v95mQN",
        "outputId": "1810d0ad-735d-4aea-9b92-9d5d26bd9e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "drive.mount(\"/content/drive/\")\n",
        "\n",
        "JSON_FILE_PATH = \"drive/MyDrive/NLP/\"\n",
        "JSON_FILE_NAME = \"News_Category_Dataset_v3_balanced.json\";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4D9p5eqOFzq"
      },
      "source": [
        "Extract the data from the Json file and turn it into a panda DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-8hrAt937mT4"
      },
      "outputs": [],
      "source": [
        "def extractJsonData(jsonData):\n",
        "  return pd.read_json(jsonData, lines=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AJ3KAhcu8iiO"
      },
      "outputs": [],
      "source": [
        "jsonFile = open(JSON_FILE_PATH + JSON_FILE_NAME);\n",
        "df = extractJsonData(jsonFile);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "bGIIW9qIfUtm",
        "outputId": "df220923-d11c-4d04-9faf-62906807ada2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                link  \\\n",
              "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
              "1  https://www.huffpost.com/entry/american-airlin...   \n",
              "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
              "3  https://www.huffpost.com/entry/funniest-parent...   \n",
              "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
              "\n",
              "                                            headline   category  \\\n",
              "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
              "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
              "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
              "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
              "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
              "\n",
              "                                   short_description               authors  \\\n",
              "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
              "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
              "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
              "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
              "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
              "\n",
              "        date  \n",
              "0 2022-09-23  \n",
              "1 2022-09-23  \n",
              "2 2022-09-23  \n",
              "3 2022-09-23  \n",
              "4 2022-09-22  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7e72655e-12cc-4ade-aa33-076ce51d7f25\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>link</th>\n",
              "      <th>headline</th>\n",
              "      <th>category</th>\n",
              "      <th>short_description</th>\n",
              "      <th>authors</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.huffpost.com/entry/covid-boosters-...</td>\n",
              "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
              "      <td>U.S. NEWS</td>\n",
              "      <td>Health experts said it is too early to predict...</td>\n",
              "      <td>Carla K. Johnson, AP</td>\n",
              "      <td>2022-09-23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.huffpost.com/entry/american-airlin...</td>\n",
              "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
              "      <td>U.S. NEWS</td>\n",
              "      <td>He was subdued by passengers and crew when he ...</td>\n",
              "      <td>Mary Papenfuss</td>\n",
              "      <td>2022-09-23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
              "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
              "      <td>COMEDY</td>\n",
              "      <td>\"Until you have a dog you don't understand wha...</td>\n",
              "      <td>Elyse Wanshel</td>\n",
              "      <td>2022-09-23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
              "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
              "      <td>PARENTING</td>\n",
              "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
              "      <td>Caroline Bologna</td>\n",
              "      <td>2022-09-23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.huffpost.com/entry/amy-cooper-lose...</td>\n",
              "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
              "      <td>U.S. NEWS</td>\n",
              "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
              "      <td>Nina Golgowski</td>\n",
              "      <td>2022-09-22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e72655e-12cc-4ade-aa33-076ce51d7f25')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7e72655e-12cc-4ade-aa33-076ce51d7f25 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7e72655e-12cc-4ade-aa33-076ce51d7f25');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "43DUwMi9ibLQ"
      },
      "outputs": [],
      "source": [
        "df = df.drop('link', axis=1)\n",
        "df = df.drop('authors', axis=1)\n",
        "df = df.drop('date', axis=1)\n",
        "df = df.drop('headline', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nmi_aDpPW_L"
      },
      "source": [
        "Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XQhOxg6NPa3-"
      },
      "outputs": [],
      "source": [
        "def lower_text(dataframe):\n",
        "  return dataframe[\"short_description\"].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Yuac8ywWPxB6"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "punctuation = string.punctuation\n",
        "def remove_punctuation(text):\n",
        "  return text.translate(str.maketrans('', '', punctuation))\n",
        "\n",
        "def remove_punctuation_text(dataframe):\n",
        "  return dataframe[\"short_description_pre_process\"].apply(lambda text: remove_punctuation(text));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Tls4W-dsRahP"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text):\n",
        "  return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "def remove_stopwords_text(dataframe):\n",
        "  return dataframe[\"short_description_pre_process\"].apply(lambda text: remove_stopwords(text));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rkOGMg72S4Ah"
      },
      "outputs": [],
      "source": [
        "cnt = Counter()\n",
        "def count_word_occurence_df(dataframe):\n",
        "  for text in dataframe[\"short_description_pre_process\"].values:\n",
        "    for word in text.split():\n",
        "      cnt[word] += 1;\n",
        "  return cnt;\n",
        "\n",
        "def remove_frequentWord(text, freqwords):\n",
        "  return \" \".join([word for word in str(text).split() if word not in freqwords]);\n",
        "\n",
        "def remove_frequent_word_text(dataframe):\n",
        "  freqwords = set([w for (w, wc) in cnt.most_common(20)])\n",
        "  return dataframe[\"short_description_pre_process\"].apply(lambda text: remove_frequentWord(text,freqwords))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RJOXEEciWRDa"
      },
      "outputs": [],
      "source": [
        "n_rare_words = 0\n",
        "def count_rare_word_df(dataframe):\n",
        "  rare_words = 0\n",
        "  for (w, wc) in cnt.most_common():\n",
        "    if wc == 1:\n",
        "      rare_words += 1;\n",
        "  return rare_words;\n",
        "\n",
        "def remove_rareWord(text, rare_words):\n",
        "  return \" \".join([word for word in str(text).split() if word not in rare_words]);\n",
        "\n",
        "def remove_rare_word_text(dataframe):\n",
        "  rare_words = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
        "  return dataframe[\"short_description_pre_process\"].apply(lambda text: remove_rareWord(text, rare_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-8pcbhamZZSs"
      },
      "outputs": [],
      "source": [
        "def lemmatize(dataframe):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
        "\n",
        "  def lemmatize_words(text):\n",
        "    pos_tagged_text = nltk.pos_tag(text.split())\n",
        "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
        "  \n",
        "  return dataframe[\"short_description_pre_process\"].apply(lambda text: lemmatize_words(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ms1eV7ZKcmbI"
      },
      "outputs": [],
      "source": [
        "def remove_emoji(string):\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "    u\"\\U00002702-\\U000027B0\"\n",
        "    u\"\\U000024C2-\\U0001F251\"\n",
        "    \"]+\", flags=re.UNICODE)\n",
        "  return emoji_pattern.sub(r'', string)\n",
        "\n",
        "def remove_emoji_text(dataframe):\n",
        "  return dataframe[\"short_description_pre_process\"].apply(lambda text: remove_emoji(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8s3ZRJW0jFgB"
      },
      "outputs": [],
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "def symbols_cleaning_text(dataframe):\n",
        "  return dataframe[\"short_description_pre_process\"].apply(lambda text: REPLACE_BY_SPACE_RE.sub(' ', text))\n",
        "\n",
        "def numbers_cleaning_text(dataframe):\n",
        "  return dataframe[\"short_description_pre_process\"].apply(lambda text: BAD_SYMBOLS_RE.sub('', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Q_8e1IlUdW23"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(dataframe):\n",
        "  dataframe[\"short_description_pre_process\"] = lower_text(dataframe)\n",
        "  dataframe[\"short_description_pre_process\"] = remove_punctuation_text(dataframe)\n",
        "  dataframe[\"short_description_pre_process\"] = remove_stopwords_text(dataframe)\n",
        "  dataframe[\"short_description_pre_process\"] = remove_frequent_word_text(dataframe)\n",
        "  dataframe[\"short_description_pre_process\"] = remove_rare_word_text(dataframe)\n",
        "  dataframe[\"short_description_pre_process\"] = remove_emoji_text(dataframe)\n",
        "  dataframe[\"short_description_pre_process\"] = symbols_cleaning_text(dataframe)\n",
        "  dataframe[\"short_description_pre_process\"] = numbers_cleaning_text(dataframe)\n",
        "  #dataframe[\"short_description_pre_process\"] = lemmatize(dataframe)\n",
        "  return dataframe[\"short_description_pre_process\"]\n",
        "\n",
        "df[\"short_description_pre_process\"] = preprocess_text(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vHWBrxJ435vB"
      },
      "outputs": [],
      "source": [
        "df = df.drop(df[df.short_description_pre_process == \"\"].index)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"short_description_pre_process\"].apply(lambda x: len(x.split(\" \"))).describe()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmpliF3a5Hbn",
        "outputId": "fc67e010-1817-48f1-b939-14e9c8e45e39"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    189492.000000\n",
              "mean         12.168894\n",
              "std           6.955057\n",
              "min           1.000000\n",
              "25%           7.000000\n",
              "50%          11.000000\n",
              "75%          15.000000\n",
              "max         140.000000\n",
              "Name: short_description_pre_process, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop('short_description', axis=1)"
      ],
      "metadata": {
        "id": "D3gC0AkyW2jl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = class_weight.compute_class_weight(class_weight = 'balanced',\n",
        "                                                  classes = np.unique(df.category.values),\n",
        "                                                  y = df.category.values)"
      ],
      "metadata": {
        "id": "3PjCfHlgJAJy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = dict(zip(np.unique([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]), class_weights))\n",
        "class_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7KsI_O-JHsK",
        "outputId": "218c8441-8ba0-41c1-9cb8-72fe0571f4e4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 2.1508495930806686,\n",
              " 1: 1.0196458262708443,\n",
              " 2: 1.5177816224529028,\n",
              " 3: 2.479061187644727,\n",
              " 4: 2.0485178698838946,\n",
              " 5: 3.854048447129172,\n",
              " 6: 0.4771055215650729,\n",
              " 7: 2.01210499490316,\n",
              " 8: 0.8495608548870865,\n",
              " 9: 0.65116183171481,\n",
              " 10: 1.6264709669112913,\n",
              " 11: 2.274213293007849,\n",
              " 12: 2.9266981744045966,\n",
              " 13: 3.3839065680917177,\n",
              " 14: 0.5689220348753423,\n",
              " 15: 0.21683934444238467,\n",
              " 16: 3.7410566216536365,\n",
              " 17: 1.7995441595441595,\n",
              " 18: 1.5946880759423363,\n",
              " 19: 0.6192184773444699,\n",
              " 20: 0.7452715538093047,\n",
              " 21: 5.096748164286291,\n",
              " 22: 1.9212215226450102,\n",
              " 23: 3.042142272311323,\n",
              " 24: 0.30253565920433756,\n",
              " 25: 2.2139502278303542,\n",
              " 26: 0.8559851472401784}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelBinarizer()\n",
        "df_cat_enc = encoder.fit_transform(df.category.values);\n",
        "df[\"category\"] = df_cat_enc.tolist()"
      ],
      "metadata": {
        "id": "_VTvqFdJehq9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = df.drop('new_category', axis=1)"
      ],
      "metadata": {
        "id": "dBpQswpwqjPQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-FKcmj0MgXDf",
        "outputId": "402b72a5-e7c8-48ee-bf92-91f56b681d7a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            category  \\\n",
              "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
              "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "\n",
              "                       short_description_pre_process  \n",
              "0  health experts said early predict whether dema...  \n",
              "1  subdued passengers crew fled back aircraft con...  \n",
              "2                    dog dont understand could eaten  \n",
              "3  accidentally put grownup toothpaste toddlers t...  \n",
              "4  amy cooper accused investment firm franklin te...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4958dad0-58ec-4e41-8312-e5eb7522d6d5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>short_description_pre_process</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>health experts said early predict whether dema...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>subdued passengers crew fled back aircraft con...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>dog dont understand could eaten</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "      <td>accidentally put grownup toothpaste toddlers t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>amy cooper accused investment firm franklin te...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4958dad0-58ec-4e41-8312-e5eb7522d6d5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4958dad0-58ec-4e41-8312-e5eb7522d6d5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4958dad0-58ec-4e41-8312-e5eb7522d6d5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(df, test_size=0.2, shuffle=True, stratify=df.category)\n",
        "train, validation = train_test_split(train, test_size=0.4, shuffle=True, stratify=train.category)"
      ],
      "metadata": {
        "id": "hWmm7QaHR_il"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train[\"category\"].value_counts(normalize=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQIuXfA6TuQL",
        "outputId": "bf3dbd45-e12e-46a7-8a5d-ece5652b0db5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.170810\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]    0.122423\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.077632\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.065098\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]    0.059810\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.056874\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]    0.049695\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.043593\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]    0.043263\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.036326\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.024397\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]    0.023231\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.022770\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.020582\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]    0.019273\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.018405\n",
            "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.018086\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.017217\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]    0.016734\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.016283\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.014941\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.012655\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]    0.012182\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.010939\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.009906\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.009609\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]    0.007267\n",
            "Name: category, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test[\"category\"].value_counts(normalize=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ7yG3e8U9aY",
        "outputId": "d7a9cd10-c076-4f00-9703-a482dc3af4b7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.170796\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]    0.122431\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.077627\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.065094\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]    0.059817\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.056888\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]    0.049685\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.043590\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]    0.043273\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.036333\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.024407\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]    0.023220\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.022771\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.020581\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]    0.019288\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.018417\n",
            "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.018074\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.017230\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]    0.016729\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.016280\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.014934\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.012665\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]    0.012164\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.010950\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.009895\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.009604\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]    0.007256\n",
            "Name: category, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(validation[\"category\"].value_counts(normalize=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umfFdZOgmMdK",
        "outputId": "dc1a721e-f778-4ed3-eed3-e35c7824ef05"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.170800\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]    0.122415\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.077625\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.065108\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]    0.059814\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.056879\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]    0.049705\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.043603\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]    0.043273\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.036314\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.024407\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]    0.023220\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.022774\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.020581\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]    0.019278\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.018404\n",
            "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.018074\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.017217\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]    0.016722\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.016293\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.014941\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.012649\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]    0.012171\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.010950\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.009895\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    0.009614\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]    0.007273\n",
            "Name: category, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = Dataset.from_tensor_slices((train.short_description_pre_process, train.category.tolist()));\n",
        "validation_ds = Dataset.from_tensor_slices((validation.short_description_pre_process, validation.category.tolist()));\n",
        "test_ds = Dataset.from_tensor_slices((test.short_description_pre_process, test.category.tolist()));"
      ],
      "metadata": {
        "id": "tUKDOAslVOQQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_ds = train_ds.shuffle(100000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "validation_ds = validation_ds.shuffle(70000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "qWfdmql9bt10"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.classes_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnZoHIO1llwg",
        "outputId": "bed092cb-137c-4ce8-e34d-295f7c3334e1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ARTS & CULTURE', 'BUSINESS & FINANCES', 'COMEDY', 'CRIME',\n",
              "       'DIVORCE', 'EDUCATION', 'ENTERTAINMENT', 'ENVIRONMENT',\n",
              "       'FOOD & DRINK', 'GROUPS VOICES', 'HOME & LIVING', 'IMPACT',\n",
              "       'MEDIA', 'MISCELLANEOUS', 'PARENTING', 'POLITICS', 'RELIGION',\n",
              "       'SCIENCE & TECH', 'SPORTS', 'STYLE & BEAUTY', 'TRAVEL',\n",
              "       'U.S. NEWS', 'WEDDINGS', 'WEIRD NEWS', 'WELLNESS', 'WOMEN',\n",
              "       'WORLD NEWS'], dtype='<U19')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def invert_multi_hot(encoded_labels):\n",
        "    \"\"\"Reverse a single multi-hot encoded label to a tuple of vocab terms.\"\"\"\n",
        "    hot_indices = [i for i, elem in enumerate(encoded_labels) if elem == 1]\n",
        "    return np.take(encoder.classes_, hot_indices)[0]"
      ],
      "metadata": {
        "id": "WHzFLPD8vqNs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example, label in train_ds.take(1):\n",
        "  print('text: ', example.numpy()[0])\n",
        "  print('label: ', invert_multi_hot(label.numpy()[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVX0Fug2VxY4",
        "outputId": "6adf4a76-0fe6-454c-9eda-10dc9eb81231"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  b'maybe shouldnt blame woman baby broken hazed rebuilt form mother thinskinned sometimes sanctimonious desperately insecure'\n",
            "label:  PARENTING\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for example, label in train_ds.take(1):\n",
        "  print('text: ', example.numpy())\n",
        "  print('label: ', label.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv92e4iVcL1K",
        "outputId": "03ba2aaf-5f25-4ad2-9e8a-4fe2caa1710f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  [b'needs stop ben jacobs said sentencing greg gianforte'\n",
            " b'modern life utterly dependent electricity supply becomes erratic read mother nature'\n",
            " b'one fundamental arguments knocked'\n",
            " b'this year least 46 transgender individuals country  hundreds around world  killed horrifying acts violence biden said'\n",
            " b'    '\n",
            " b'since birds grow slowly move around meat isnt soft finegrained one reasons dry white'\n",
            " b'infectious beats rhythms key good exercise music help forget exercising get caught vibe often lyrics excellent distraction'\n",
            " b'cliven bundy controversial nevada rancher center armed standoff federal officials 2014 arrested'\n",
            " b'year dont beat dont get around sending holiday card instead hug children tight focus less unearned trophies missed soccer goals family squabbles heres boastfulfree holiday'\n",
            " b'like read sign huffpost hill get cheeky dose political news every evening trump transition'\n",
            " b'triggers incredible ally triggers lead needs healing instead ashamed triggered get excited clue work triggers lead path need go'\n",
            " b'two food preservation experts demonstrate safely fresh sardines including basic primer pressure canner'\n",
            " b'san jose estimated 17 billion treasure missing 300 years'\n",
            " b'standardized testing perpetuates inequalities designed measure'\n",
            " b'music needs heard concert fully appreciated amount spin doctoring journalists peer pressure fanatical fans justify act unless one catches said act live'\n",
            " b'thanks' b'sandler were coming'\n",
            " b'facebook ceo finally apologized data breach talked regulating company'\n",
            " b'paul zizka got idea epic photo series earlymorning mountaineering trips took kid amazed'\n",
            " b'i happen disagree pope one'\n",
            " b'doesnt matter work every day weekend warrior odds point ended tight sore muscles important master art selfmassage'\n",
            " b'moonlight actor tackles heavyweight champs triumphs troubling controversies mike unauthorized bioseries due aug 25'\n",
            " b'award show air june 18'\n",
            " b'many liberal millennials feeling bern burned american dream'\n",
            " b'one said eat turkey holidays'\n",
            " b'ally former fbi director james comey center bureaus biggest cases recent years'\n",
            " b'think capers cheesy sauce creme fraiche caviar sorry well calm'\n",
            " b'criticized obamas mosque visit divisive rubios rhetoric muslims real problem'\n",
            " b'since penn state scandal questions protecting children sexual abuse seem parents minds time'\n",
            " b'one rare weeks washington congress deigns actually job vote stuff lapsing back default status course taking weeks weeks vacation'\n",
            " b'alcohol used describe makeup come mean booze'\n",
            " b'prince passed away april 21 died one states doesnt explicit statute punishes']\n",
            "label:  [[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_WORDS = 20000\n",
        "MAX_LEN = 11 # 50% of the dataset have an average length of 11 words\n",
        "vector = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=MAX_WORDS,\n",
        "    standardize=None,\n",
        "    split=\"whitespace\",\n",
        "    ngrams=2\n",
        "  )\n",
        "vector.adapt(train_ds.map(lambda text, label: text))"
      ],
      "metadata": {
        "id": "FIHsy6Z7vG2j"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = dict()\n",
        "path_to_glove_embed = \"drive/MyDrive/NLP/glove.6B.100d.txt\"\n",
        "def get_embeddings_from_glove(glove_path):\n",
        "  embeddings_index = dict();\n",
        "  for line in open(glove_path, encoding=\"utf8\"):\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "  return embeddings_index;\n",
        "\n",
        "embeddings_index = get_embeddings_from_glove(path_to_glove_embed);"
      ],
      "metadata": {
        "id": "zGrzBXY7iBjL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding_matrix():\n",
        "  embedding_matrix = np.zeros((len(vector.get_vocabulary()), 100))\n",
        "  for word, i in zip(vector.get_vocabulary(),range(len(vector.get_vocabulary()))):\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "  return embedding_matrix\n",
        "\n",
        "embedding_matrix = get_embedding_matrix()"
      ],
      "metadata": {
        "id": "KStoF4PcjoCg"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = np.array(vector.get_vocabulary())\n",
        "print(len(vocab))\n",
        "vocab[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUKFTdQjvS8A",
        "outputId": "be65f2cc-ab34-4754-a42e-e68f9a3a0a2d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'one', 'new', 'us', 'people', 'time', 'like', 'said',\n",
              "       'get', 'day', 'life', 'would', 'many', 'make', 'dont', 'years',\n",
              "       'first', 'want', 'know'], dtype='<U35')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample_weights = class_weight.compute_sample_weight( 'balanced', Y_train )"
      ],
      "metadata": {
        "id": "lj2kgYLzIQ8r"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLjquVocOYLy"
      },
      "source": [
        "Splitting the dataset between train, validation and test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Sequential([\n",
        "#     Dense(524, activation='relu'),\n",
        "#     Dense(262, activation='relu'),\n",
        "#     Dense(Y.shape[1], activation='sigmoid')\n",
        "# ])"
      ],
      "metadata": {
        "id": "6Ruh6P49OVEQ"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "Y1m462cmlQLg"
      },
      "outputs": [],
      "source": [
        "# model = Sequential([\n",
        "#     vector,\n",
        "#     Embedding(len(vocab), EMBEDDING_DIM, mask_zero=True),\n",
        "#     GRU(EMBEDDING_DIM, return_sequences=True),\n",
        "#     GRU(int(EMBEDDING_DIM/2)),\n",
        "#     Dense(int(EMBEDDING_DIM/2), activation='relu'),\n",
        "#     Dropout(0.3),\n",
        "#     Dense(len(encoder.classes_), activation='sigmoid')\n",
        "# ])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Sequential([\n",
        "#     vector,\n",
        "#     Embedding(MAX_NB_WORDS, EMBEDDING_DIM, mask_zero=True),\n",
        "#     Conv1D(128, 5, activation='relu'),\n",
        "#     GlobalMaxPooling1D(),\n",
        "#     Dense(28, activation='relu'),\n",
        "#     Dropout(0.2),\n",
        "#     Dense(len(encoder.classes_))\n",
        "# ])\n"
      ],
      "metadata": {
        "id": "P_EgV6HlPtAO"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "06Qd1B3lrXG4"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    vector,\n",
        "    Embedding( \n",
        "        input_dim=len(vocab), \n",
        "        output_dim=100,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=MAX_LEN,\n",
        "        trainable=False\n",
        "    ),  \n",
        "    SpatialDropout1D(0.2),\n",
        "    GRU(64, dropout=0.2, return_sequences=True),  \n",
        "    BatchNormalization(),\n",
        "    GRU(64, dropout=0.2),\n",
        "    BatchNormalization(),\n",
        "    Dense(len(encoder.classes_), activation=\"sigmoid\")\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([layer.supports_masking for layer in model.layers])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybtwSnhnYtN5",
        "outputId": "518cd973-50ac-4e3b-c6e3-04f4be0532f5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False, False, True, True, True, True, True, True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXIhH5UIYyO_",
        "outputId": "4e4908c2-94a5-480f-b84d-08295ecf6a1c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "[0.5000374  0.499967   0.49995926 0.49996588 0.50003153 0.499988\n",
            " 0.50004715 0.5000145  0.49997455 0.5000109  0.4999949  0.5000078\n",
            " 0.50006384 0.5000598  0.50004166 0.5000127  0.49991375 0.50006753\n",
            " 0.50008434 0.499963   0.49997386 0.5000147  0.4999967  0.50000495\n",
            " 0.49997422 0.5000115  0.500022  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padding = \"the \" * 2000\n",
        "predictions = model.predict(np.array([sample_text, padding]))\n",
        "print(predictions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gz_64KYY-Qa",
        "outputId": "7b752b50-d8ec-4f7e-dfc7-177d9227bcb3"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 489ms/step\n",
            "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
            " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(  loss='binary_crossentropy', \n",
        "                optimizer='adam', \n",
        "                weighted_metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "qvOayJrNZFDP"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEvEn9F7P4Hu",
        "outputId": "295664e8-adc1-47da-aa94-2d59784faa6d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, None, 100)         2000000   \n",
            "                                                                 \n",
            " spatial_dropout1d_3 (Spatia  (None, None, 100)        0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " gru_6 (GRU)                 (None, None, 64)          31872     \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, None, 64)         256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " gru_7 (GRU)                 (None, 64)                24960     \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 64)               256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 27)                1755      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,059,099\n",
            "Trainable params: 58,843\n",
            "Non-trainable params: 2,000,256\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbN-te7lmSXt"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds, \n",
        "    epochs=epochs,\n",
        "    validation_data=validation_ds,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "1Zc8koftmUQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ebc9f4-7a1c-4957-d1c9-8964dbc819d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1185/1185 [==============================] - 5s 4ms/step - loss: 0.1094 - categorical_accuracy: 0.4480\n",
            "Test set\n",
            "  Loss: 0.109\n",
            "  Accuracy: 0.448\n"
          ]
        }
      ],
      "source": [
        "accr = model.evaluate(test_ds)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dB41jwPhm67h"
      },
      "outputs": [],
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1cNx-Qpm9ER"
      },
      "outputs": [],
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(history.history['categorical_accuracy'], label='train')\n",
        "plt.plot(history.history['val_categorical_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ax-IF4mm-hd"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"drive/MyDrive/NLP/training_2/cp-{epoch:04d}.ckpt\"\n",
        "model.save_weights(checkpoint_path.format(epoch=0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZC7GvDcGB6s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyPZthiTMp6jkys6wLf9bKuT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}